{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performing hyperparameter optimization in the bench class\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this example we will use a model that consumes pre-computed features.\n",
    "We will use [CatBoost](https://catboost.ai/) which you need to install separately (e.g. using `pip install catboost`).\n",
    "\n",
    "We will use [Optuna](https://optuna.org/) for hyperparameter optimization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from mofdscribe.bench import LogkHCO2OODBench\n",
    "from mofdscribe.bench.df_model import DFModel\n",
    "from mofdscribe.splitters import HashSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES = [\n",
    "    \"total_POV_gravimetric\",\n",
    "    \"mc_CRY-chi-0-all\",\n",
    "    \"mc_CRY-chi-1-all\",\n",
    "    \"mc_CRY-chi-2-all\",\n",
    "    \"mc_CRY-chi-3-all\",\n",
    "    \"mc_CRY-Z-0-all\",\n",
    "    \"mc_CRY-Z-1-all\",\n",
    "    \"mc_CRY-Z-2-all\",\n",
    "    \"mc_CRY-Z-3-all\",\n",
    "    \"mc_CRY-I-0-all\",\n",
    "    \"mc_CRY-I-1-all\",\n",
    "    \"mc_CRY-I-2-all\",\n",
    "    \"mc_CRY-I-3-all\",\n",
    "    \"mc_CRY-T-0-all\",\n",
    "    \"mc_CRY-T-1-all\",\n",
    "    \"mc_CRY-T-2-all\",\n",
    "    \"mc_CRY-T-3-all\",\n",
    "    \"mc_CRY-S-0-all\",\n",
    "    \"mc_CRY-S-1-all\",\n",
    "    \"mc_CRY-S-2-all\",\n",
    "    \"mc_CRY-S-3-all\",\n",
    "    \"D_mc_CRY-chi-0-all\",\n",
    "    \"D_mc_CRY-chi-1-all\",\n",
    "    \"D_mc_CRY-chi-2-all\",\n",
    "    \"D_mc_CRY-chi-3-all\",\n",
    "    \"D_mc_CRY-Z-0-all\",\n",
    "    \"D_mc_CRY-Z-1-all\",\n",
    "    \"D_mc_CRY-Z-2-all\",\n",
    "    \"D_mc_CRY-Z-3-all\",\n",
    "    \"D_mc_CRY-I-0-all\",\n",
    "    \"D_mc_CRY-I-1-all\",\n",
    "    \"D_mc_CRY-I-2-all\",\n",
    "    \"D_mc_CRY-I-3-all\",\n",
    "    \"D_mc_CRY-T-0-all\",\n",
    "    \"D_mc_CRY-T-1-all\",\n",
    "    \"D_mc_CRY-T-2-all\",\n",
    "    \"D_mc_CRY-T-3-all\",\n",
    "    \"D_mc_CRY-S-0-all\",\n",
    "    \"D_mc_CRY-S-1-all\",\n",
    "    \"D_mc_CRY-S-2-all\",\n",
    "    \"D_mc_CRY-S-3-all\",\n",
    "    \"sum-mc_CRY-chi-0-all\",\n",
    "    \"sum-mc_CRY-chi-1-all\",\n",
    "    \"sum-mc_CRY-chi-2-all\",\n",
    "    \"sum-mc_CRY-chi-3-all\",\n",
    "    \"sum-mc_CRY-Z-0-all\",\n",
    "    \"sum-mc_CRY-Z-1-all\",\n",
    "    \"sum-mc_CRY-Z-2-all\",\n",
    "    \"sum-mc_CRY-Z-3-all\",\n",
    "    \"sum-mc_CRY-I-0-all\",\n",
    "    \"sum-mc_CRY-I-1-all\",\n",
    "    \"sum-mc_CRY-I-2-all\",\n",
    "    \"sum-mc_CRY-I-3-all\",\n",
    "    \"sum-mc_CRY-T-0-all\",\n",
    "    \"sum-mc_CRY-T-1-all\",\n",
    "    \"sum-mc_CRY-T-2-all\",\n",
    "    \"sum-mc_CRY-T-3-all\",\n",
    "    \"sum-mc_CRY-S-0-all\",\n",
    "    \"sum-mc_CRY-S-1-all\",\n",
    "    \"sum-mc_CRY-S-2-all\",\n",
    "    \"sum-mc_CRY-S-3-all\",\n",
    "    \"sum-D_mc_CRY-chi-0-all\",\n",
    "    \"sum-D_mc_CRY-chi-1-all\",\n",
    "    \"sum-D_mc_CRY-chi-2-all\",\n",
    "    \"sum-D_mc_CRY-chi-3-all\",\n",
    "    \"sum-D_mc_CRY-Z-0-all\",\n",
    "    \"sum-D_mc_CRY-Z-1-all\",\n",
    "    \"sum-D_mc_CRY-Z-2-all\",\n",
    "    \"sum-D_mc_CRY-Z-3-all\",\n",
    "    \"sum-D_mc_CRY-I-0-all\",\n",
    "    \"sum-D_mc_CRY-I-1-all\",\n",
    "    \"sum-D_mc_CRY-I-2-all\",\n",
    "    \"sum-D_mc_CRY-I-3-all\",\n",
    "    \"sum-D_mc_CRY-T-0-all\",\n",
    "    \"sum-D_mc_CRY-T-1-all\",\n",
    "    \"sum-D_mc_CRY-T-2-all\",\n",
    "    \"sum-D_mc_CRY-T-3-all\",\n",
    "    \"sum-D_mc_CRY-S-0-all\",\n",
    "    \"sum-D_mc_CRY-S-1-all\",\n",
    "    \"sum-D_mc_CRY-S-2-all\",\n",
    "    \"sum-D_mc_CRY-S-3-all\",\n",
    "    \"D_lc-chi-0-all\",\n",
    "    \"D_lc-chi-1-all\",\n",
    "    \"D_lc-chi-2-all\",\n",
    "    \"D_lc-chi-3-all\",\n",
    "    \"D_lc-Z-0-all\",\n",
    "    \"D_lc-Z-1-all\",\n",
    "    \"D_lc-Z-2-all\",\n",
    "    \"D_lc-Z-3-all\",\n",
    "    \"D_lc-I-0-all\",\n",
    "    \"D_lc-I-1-all\",\n",
    "    \"D_lc-I-2-all\",\n",
    "    \"D_lc-I-3-all\",\n",
    "    \"D_lc-T-0-all\",\n",
    "    \"D_lc-T-1-all\",\n",
    "    \"D_lc-T-2-all\",\n",
    "    \"D_lc-T-3-all\",\n",
    "    \"D_lc-S-0-all\",\n",
    "    \"D_lc-S-1-all\",\n",
    "    \"D_lc-S-2-all\",\n",
    "    \"D_lc-S-3-all\",\n",
    "    \"D_lc-alpha-0-all\",\n",
    "    \"D_lc-alpha-1-all\",\n",
    "    \"D_lc-alpha-2-all\",\n",
    "    \"D_lc-alpha-3-all\",\n",
    "    \"D_func-chi-0-all\",\n",
    "    \"D_func-chi-1-all\",\n",
    "    \"D_func-chi-2-all\",\n",
    "    \"D_func-chi-3-all\",\n",
    "    \"D_func-Z-0-all\",\n",
    "    \"D_func-Z-1-all\",\n",
    "    \"D_func-Z-2-all\",\n",
    "    \"D_func-Z-3-all\",\n",
    "    \"D_func-I-0-all\",\n",
    "    \"D_func-I-1-all\",\n",
    "    \"D_func-I-2-all\",\n",
    "    \"D_func-I-3-all\",\n",
    "    \"D_func-T-0-all\",\n",
    "    \"D_func-T-1-all\",\n",
    "    \"D_func-T-2-all\",\n",
    "    \"D_func-T-3-all\",\n",
    "    \"D_func-S-0-all\",\n",
    "    \"D_func-S-1-all\",\n",
    "    \"D_func-S-2-all\",\n",
    "    \"D_func-S-3-all\",\n",
    "    \"D_func-alpha-0-all\",\n",
    "    \"D_func-alpha-1-all\",\n",
    "    \"D_func-alpha-2-all\",\n",
    "    \"D_func-alpha-3-all\",\n",
    "    \"sum-D_lc-chi-0-all\",\n",
    "    \"sum-D_lc-chi-1-all\",\n",
    "    \"sum-D_lc-chi-2-all\",\n",
    "    \"sum-D_lc-chi-3-all\",\n",
    "    \"sum-D_lc-Z-0-all\",\n",
    "    \"sum-D_lc-Z-1-all\",\n",
    "    \"sum-D_lc-Z-2-all\",\n",
    "    \"sum-D_lc-Z-3-all\",\n",
    "    \"sum-D_lc-I-0-all\",\n",
    "    \"sum-D_lc-I-1-all\",\n",
    "    \"sum-D_lc-I-2-all\",\n",
    "    \"sum-D_lc-I-3-all\",\n",
    "    \"sum-D_lc-T-0-all\",\n",
    "    \"sum-D_lc-T-1-all\",\n",
    "    \"sum-D_lc-T-2-all\",\n",
    "    \"sum-D_lc-T-3-all\",\n",
    "    \"sum-D_lc-S-0-all\",\n",
    "    \"sum-D_lc-S-1-all\",\n",
    "    \"sum-D_lc-S-2-all\",\n",
    "    \"sum-D_lc-S-3-all\",\n",
    "    \"sum-D_lc-alpha-0-all\",\n",
    "    \"sum-D_lc-alpha-1-all\",\n",
    "    \"sum-D_lc-alpha-2-all\",\n",
    "    \"sum-D_lc-alpha-3-all\",\n",
    "    \"sum-D_func-chi-0-all\",\n",
    "    \"sum-D_func-chi-1-all\",\n",
    "    \"sum-D_func-chi-2-all\",\n",
    "    \"sum-D_func-chi-3-all\",\n",
    "    \"sum-D_func-Z-0-all\",\n",
    "    \"sum-D_func-Z-1-all\",\n",
    "    \"sum-D_func-Z-2-all\",\n",
    "    \"sum-D_func-Z-3-all\",\n",
    "    \"sum-D_func-I-0-all\",\n",
    "    \"sum-D_func-I-1-all\",\n",
    "    \"sum-D_func-I-2-all\",\n",
    "    \"sum-D_func-I-3-all\",\n",
    "    \"sum-D_func-T-0-all\",\n",
    "    \"sum-D_func-T-1-all\",\n",
    "    \"sum-D_func-T-2-all\",\n",
    "    \"sum-D_func-T-3-all\",\n",
    "    \"sum-D_func-S-0-all\",\n",
    "    \"sum-D_func-S-1-all\",\n",
    "    \"sum-D_func-S-2-all\",\n",
    "    \"sum-D_func-S-3-all\",\n",
    "    \"sum-D_func-alpha-0-all\",\n",
    "    \"sum-D_func-alpha-1-all\",\n",
    "    \"sum-D_func-alpha-2-all\",\n",
    "    \"sum-D_func-alpha-3-all\",\n",
    "]\n",
    "\n",
    "TARGET = \"logKH_CO2\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make sure we can make it work outside MOFBench\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune(train_data, valid_data, num_trials=10):\n",
    "    def objective(trial):\n",
    "        param = {\n",
    "            \"colsample_bylevel\": trial.suggest_float(\"colsample_bylevel\", 0.01, 0.1, log=True),\n",
    "            \"depth\": trial.suggest_int(\"depth\", 1, 16),\n",
    "            \"iterations\": trial.suggest_int(\"iterations\", 1, 10000),\n",
    "            \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.001, 0.5, log=True),\n",
    "            \"l2_leaf_reg\": trial.suggest_float(\"l2_leaf_reg\", 0.01, 10),\n",
    "            \"random_strength\": trial.suggest_float(\"random_strength\", 0.01, 10),\n",
    "            \"bagging_temperature\": trial.suggest_float(\"bagging_temperature\", 0.01, 10),\n",
    "        }\n",
    "        model = CatBoostRegressor(\n",
    "            **param,\n",
    "            silent=True,\n",
    "        )\n",
    "        model.fit(train_data[0], train_data[1])\n",
    "\n",
    "        predictions = model.predict(valid_data[0])\n",
    "        mse = mean_squared_error(valid_data[1], predictions)\n",
    "        return mse\n",
    "\n",
    "    study = optuna.create_study(\n",
    "        pruner=optuna.pruners.MedianPruner(n_warmup_steps=5), direction=\"minimize\"\n",
    "    )\n",
    "    study.optimize(\n",
    "        objective,\n",
    "        n_trials=num_trials,\n",
    "        timeout=600,\n",
    "        callbacks=[],  # WeightsAndBiasesCallback(wandb_kwargs==wandb_kwargs) can be nice to use\n",
    "    )\n",
    "    model = CatBoostRegressor(\n",
    "        **study.best_params,\n",
    "        silent=True,\n",
    "    )\n",
    "\n",
    "    model.fit(train_data[0], train_data[1])\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following lines are solely for the purpose of debugging!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-04 15:27:15.742 | DEBUG    | mofdscribe.datasets.core_dataset:__init__:123 - Dropped 639 duplicate basenames. New length 8182\n",
      "2022-08-04 15:27:15.751 | DEBUG    | mofdscribe.datasets.core_dataset:__init__:129 - Dropped 1312 duplicate graphs. New length 6870\n",
      "2022-08-04 15:27:16.671 | DEBUG    | mofdscribe.datasets.core_dataset:__init__:123 - Dropped 639 duplicate basenames. New length 8182\n",
      "2022-08-04 15:27:16.682 | DEBUG    | mofdscribe.datasets.core_dataset:__init__:129 - Dropped 1312 duplicate graphs. New length 6870\n",
      "2022-08-04 15:27:16.704 | DEBUG    | mofdscribe.splitters.splitters:__init__:116 - Splitter settings | shuffle True, random state None, sample frac 1.0, q (0, 0.25, 0.5, 0.75, 1)\n"
     ]
    }
   ],
   "source": [
    "df = LogkHCO2OODBench(None)._ds._df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "part_1 = df.iloc[:100]\n",
    "part_2 = df.iloc[100:200]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-08-04 15:27:16,797]\u001b[0m A new study created in memory with name: no-name-ae210727-6a82-46ef-ad1a-cbbc2c7c86f1\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "model = tune((part_1[FEATURES], part_1[TARGET]), (part_2[FEATURES], part_2[TARGET]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seems to work.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how we implement this in a `MOFBench` class using a `mofdscribe` splitter.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example we really want to avoid data leakage and hence will also use the `HashSplitter` in the inner loop.\n",
    "Doing so is relatively easy as we can construct new datasets that we can use in splitters using the `get_subset` method of the datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCatBoostModel:\n",
    "    def __init__(self, features=FEATURES):\n",
    "        self.features = features\n",
    "        self.model = CatBoostRegressor()\n",
    "\n",
    "    def tune(self, idx, y):\n",
    "        tune_splitter = HashSplitter(self.ds.get_subset(idx))\n",
    "        # we will now use a simple split in two parts,\n",
    "        # however, you could also use a k-fold in the tune method\n",
    "        train_idx, valid_idx = tune_splitter.split(0.7)\n",
    "        train_idx = idx[train_idx]\n",
    "        valid_idx = idx[valid_idx]\n",
    "        train_data = (self.ds._df.iloc[train_idx][self.features], self.ds._df.iloc[train_idx][y])\n",
    "        valid_data = (self.ds._df.iloc[valid_idx][self.features], self.ds._df.iloc[valid_idx][y])\n",
    "        self.model = tune(train_data, valid_data)\n",
    "\n",
    "    def fit(self, idx, structures, y):\n",
    "        self.tune(idx, y)\n",
    "        X = self.ds._df.iloc[idx][self.features]\n",
    "        self.model.fit(X, y)\n",
    "\n",
    "    def predict(self, idx, structures):\n",
    "        X = self.ds._df.iloc[idx][self.features]\n",
    "        pred =  self.model.predict(X)\n",
    "        print(pred)\n",
    "        return pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-04 15:25:44.029 | DEBUG    | mofdscribe.datasets.core_dataset:__init__:119 - Dropped 639 duplicate basenames. New length 8182\n",
      "2022-08-04 15:25:44.039 | DEBUG    | mofdscribe.datasets.core_dataset:__init__:125 - Dropped 1312 duplicate graphs. New length 6870\n",
      "2022-08-04 15:25:44.963 | DEBUG    | mofdscribe.datasets.core_dataset:__init__:119 - Dropped 639 duplicate basenames. New length 8182\n",
      "2022-08-04 15:25:44.973 | DEBUG    | mofdscribe.datasets.core_dataset:__init__:125 - Dropped 1312 duplicate graphs. New length 6870\n",
      "2022-08-04 15:25:44.995 | DEBUG    | mofdscribe.splitters.splitters:__init__:116 - Splitter settings | shuffle True, random state None, sample frac 0.01, q (0, 0.25, 0.5, 0.75, 1)\n"
     ]
    }
   ],
   "source": [
    "bench = LogkHCO2OODBench(MyCatBoostModel(), patch_in_ds=True, debug=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-04 15:26:54.826 | DEBUG    | mofdscribe.bench.mofbench:_score:230 - K-fold round 0, 54 train points, 13 test points\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'CoREDataset' object has no attribute '_drop_basename_duplicates'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/kevinmaikjablonka/git/kjappelbaum/mofdscribe/examples/hyperparameter_optimization_in_bench.ipynb Cell 16\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/kevinmaikjablonka/git/kjappelbaum/mofdscribe/examples/hyperparameter_optimization_in_bench.ipynb#ch0000016?line=0'>1</a>\u001b[0m result \u001b[39m=\u001b[39m bench\u001b[39m.\u001b[39;49mbench()\n",
      "File \u001b[0;32m~/git/kjappelbaum/mofdscribe/src/mofdscribe/bench/mofbench.py:200\u001b[0m, in \u001b[0;36mMOFBench.bench\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[39m\"\"\"Run the benchmarking.\"\"\"\u001b[39;00m\n\u001b[1;32m    199\u001b[0m start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m--> 200\u001b[0m metrics \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_score()\n\u001b[1;32m    201\u001b[0m end_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m    202\u001b[0m \u001b[39mreturn\u001b[39;00m BenchResult(\n\u001b[1;32m    203\u001b[0m     start_time\u001b[39m=\u001b[39mstart_time,\n\u001b[1;32m    204\u001b[0m     end_time\u001b[39m=\u001b[39mend_time,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    213\u001b[0m     mofdscribe_version\u001b[39m=\u001b[39mget_version(),\n\u001b[1;32m    214\u001b[0m )\n",
      "File \u001b[0;32m~/git/kjappelbaum/mofdscribe/src/mofdscribe/bench/mofbench.py:234\u001b[0m, in \u001b[0;36mMOFBenchRegression._score\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    230\u001b[0m logger\u001b[39m.\u001b[39mdebug(\n\u001b[1;32m    231\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mK-fold round \u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(train_idx)\u001b[39m}\u001b[39;00m\u001b[39m train points, \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(test_idx)\u001b[39m}\u001b[39;00m\u001b[39m test points\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    232\u001b[0m )\n\u001b[1;32m    233\u001b[0m start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m--> 234\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_train(\n\u001b[1;32m    235\u001b[0m     train_idx,\n\u001b[1;32m    236\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_ds\u001b[39m.\u001b[39;49mget_structures(train_idx),\n\u001b[1;32m    237\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_ds\u001b[39m.\u001b[39;49m_df[\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_targets]\u001b[39m.\u001b[39;49miloc[train_idx]\u001b[39m.\u001b[39;49mvalues,\n\u001b[1;32m    238\u001b[0m )\n\u001b[1;32m    239\u001b[0m end_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m    240\u001b[0m timings\u001b[39m.\u001b[39mappend(end_time \u001b[39m-\u001b[39m start_time)\n",
      "File \u001b[0;32m~/git/kjappelbaum/mofdscribe/src/mofdscribe/bench/mofbench.py:187\u001b[0m, in \u001b[0;36mMOFBench._train\u001b[0;34m(self, idx, structures, y)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_train\u001b[39m(\u001b[39mself\u001b[39m, idx: np\u001b[39m.\u001b[39mndarray, structures: np\u001b[39m.\u001b[39mndarray, y: np\u001b[39m.\u001b[39mndarray):\n\u001b[0;32m--> 187\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_model\u001b[39m.\u001b[39;49mfit(idx, structures, y)\n\u001b[1;32m    188\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fitted \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[1;32m/Users/kevinmaikjablonka/git/kjappelbaum/mofdscribe/examples/hyperparameter_optimization_in_bench.ipynb Cell 16\u001b[0m in \u001b[0;36mMyCatBoostModel.fit\u001b[0;34m(self, idx, structures, y)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kevinmaikjablonka/git/kjappelbaum/mofdscribe/examples/hyperparameter_optimization_in_bench.ipynb#ch0000016?line=16'>17</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit\u001b[39m(\u001b[39mself\u001b[39m, idx, structures, y):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/kevinmaikjablonka/git/kjappelbaum/mofdscribe/examples/hyperparameter_optimization_in_bench.ipynb#ch0000016?line=17'>18</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtune(idx, y)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kevinmaikjablonka/git/kjappelbaum/mofdscribe/examples/hyperparameter_optimization_in_bench.ipynb#ch0000016?line=18'>19</a>\u001b[0m     X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mds\u001b[39m.\u001b[39m_df\u001b[39m.\u001b[39miloc[idx][\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeatures]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kevinmaikjablonka/git/kjappelbaum/mofdscribe/examples/hyperparameter_optimization_in_bench.ipynb#ch0000016?line=19'>20</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mfit(X, y)\n",
      "\u001b[1;32m/Users/kevinmaikjablonka/git/kjappelbaum/mofdscribe/examples/hyperparameter_optimization_in_bench.ipynb Cell 16\u001b[0m in \u001b[0;36mMyCatBoostModel.tune\u001b[0;34m(self, idx, y)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/kevinmaikjablonka/git/kjappelbaum/mofdscribe/examples/hyperparameter_optimization_in_bench.ipynb#ch0000016?line=5'>6</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtune\u001b[39m(\u001b[39mself\u001b[39m, idx, y):\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/kevinmaikjablonka/git/kjappelbaum/mofdscribe/examples/hyperparameter_optimization_in_bench.ipynb#ch0000016?line=6'>7</a>\u001b[0m     tune_splitter \u001b[39m=\u001b[39m HashSplitter(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mds\u001b[39m.\u001b[39;49mget_subset(idx))\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/kevinmaikjablonka/git/kjappelbaum/mofdscribe/examples/hyperparameter_optimization_in_bench.ipynb#ch0000016?line=7'>8</a>\u001b[0m     \u001b[39m# we will now use a simple split in two parts,\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/kevinmaikjablonka/git/kjappelbaum/mofdscribe/examples/hyperparameter_optimization_in_bench.ipynb#ch0000016?line=8'>9</a>\u001b[0m     \u001b[39m# however, you could also use a k-fold in the tune method\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kevinmaikjablonka/git/kjappelbaum/mofdscribe/examples/hyperparameter_optimization_in_bench.ipynb#ch0000016?line=9'>10</a>\u001b[0m     train_idx, valid_idx \u001b[39m=\u001b[39m tune_splitter\u001b[39m.\u001b[39msplit(\u001b[39m0.7\u001b[39m)\n",
      "File \u001b[0;32m~/git/kjappelbaum/mofdscribe/src/mofdscribe/datasets/core_dataset.py:173\u001b[0m, in \u001b[0;36mget_subset\u001b[0;34m(self, indices)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_subset\u001b[39m(\u001b[39mself\u001b[39m, indices: Iterable[\u001b[39mint\u001b[39m]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mStructureDataset\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    164\u001b[0m     \u001b[39m\"\"\"Get a subset of the dataset.\u001b[39;00m\n\u001b[1;32m    165\u001b[0m \n\u001b[1;32m    166\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[39m            specified by the indices.\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m     \u001b[39mreturn\u001b[39;00m CoREDataset(\n\u001b[1;32m    174\u001b[0m         version\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mversion,\n\u001b[1;32m    175\u001b[0m         drop_basename_duplicates\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_drop_basename_duplicates,\n\u001b[1;32m    176\u001b[0m         drop_graph_duplicates\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_drop_graph_duplicates,\n\u001b[1;32m    177\u001b[0m         subset\u001b[39m=\u001b[39mindices,\n\u001b[1;32m    178\u001b[0m     )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'CoREDataset' object has no attribute '_drop_basename_duplicates'"
     ]
    }
   ],
   "source": [
    "result = bench.bench()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('mofdscribe')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3ffc06f754d7c80b59e39914e7792f1f92938dc6ca13a8ff96847f8f4d27fee3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
