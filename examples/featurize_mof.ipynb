{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to submit to the leaderboard\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we will build a graph-convolutional neural network for the PBE bandgap task.\n",
    "We will use the [Crystal Graph Convolutional Neural Network](https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.120.145301) implemented in the [Deepchem library](https://deepchem.io/).\n",
    "\n",
    "We choose this one, as it will need some more involved training loop and customization that you might also need in a real-world scenario.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To install Deepchem, follow the installation instructions on [the Deepchem landing page](https://deepchem.io/). You'll need to run something along the following lines (and also install PyTorch and dgl):\n",
    "\n",
    "> conda install -c conda-forge rdkit deepchem==2.6.1\n",
    ">\n",
    "> pip install tensorflow-gpu~=2.4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepchem.data import Dataset\n",
    "from deepchem.data import DiskDataset\n",
    "from deepchem.data.data_loader import DataLoader\n",
    "from deepchem.feat import CGCNNFeaturizer\n",
    "from deepchem.models.torch_models.cgcnn import CGCNNModel\n",
    "from deepchem.molnet.load_function.molnet_loader import TransformerGenerator, _MolnetLoader\n",
    "from deepchem.molnet.load_function.molnet_loader import TransformerGenerator, _MolnetLoader\n",
    "from loguru import logger\n",
    "from mofdscribe.bench import PBEBandGapBench\n",
    "from os import PathLike\n",
    "from os.path import join\n",
    "from pymatgen.core import Structure\n",
    "from typing import Any, Iterator, List, Optional, Sequence, Tuple, Union, Iterable\n",
    "import concurrent.futures\n",
    "import deepchem as dc\n",
    "import json\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "logger = logging.getLogger(__name__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DGL backend not selected or invalid.  Assuming PyTorch for now.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting the default backend to \"pytorch\". You can change it in the ~/.dgl/config.json file or export the DGLBACKEND environment variable.  Valid options are: pytorch, mxnet, tensorflow (all lowercase)\n"
     ]
    }
   ],
   "source": [
    "model = CGCNNModel(mode=\"regression\")  # we use default settings for demonstration purposes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we have a custom dataloader, which also supports multiprocessing. However, you could, of course, also use the default `InMemoryLoader`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InMemoryLoader(DataLoader):\n",
    "    \"\"\"Facilitate Featurization of In-memory objects.\n",
    "\n",
    "    When featurizing a dataset, it's often the case that the initial set of\n",
    "    data (pre-featurization) fits handily within memory. (For example, perhaps\n",
    "    it fits within a column of a pandas DataFrame.) In this case, it would be\n",
    "    convenient to directly be able to featurize this column of data. However,\n",
    "    the process of featurization often generates large arrays which quickly eat\n",
    "    up available memory. This class provides convenient capabilities to process\n",
    "    such in-memory data by checkpointing generated features periodically to\n",
    "    disk.\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    Here's an example with only datapoints and no labels or weights.\n",
    "\n",
    "    >>> import deepchem as dc\n",
    "    >>> smiles = [\"C\", \"CC\", \"CCC\", \"CCCC\"]\n",
    "    >>> featurizer = dc.feat.CircularFingerprint()\n",
    "    >>> loader = dc.data.InMemoryLoader(tasks=[\"task1\"], featurizer=featurizer)\n",
    "    >>> dataset = loader.create_dataset(smiles, shard_size=2)\n",
    "    >>> len(dataset)\n",
    "    4\n",
    "\n",
    "    Here's an example with both datapoints and labels\n",
    "\n",
    "    >>> import deepchem as dc\n",
    "    >>> smiles = [\"C\", \"CC\", \"CCC\", \"CCCC\"]\n",
    "    >>> labels = [1, 0, 1, 0]\n",
    "    >>> featurizer = dc.feat.CircularFingerprint()\n",
    "    >>> loader = dc.data.InMemoryLoader(tasks=[\"task1\"], featurizer=featurizer)\n",
    "    >>> dataset = loader.create_dataset(zip(smiles, labels), shard_size=2)\n",
    "    >>> len(dataset)\n",
    "    4\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def create_dataset(\n",
    "        self,\n",
    "        inputs: Sequence[Any],\n",
    "        data_dir: Optional[str] = None,\n",
    "        shard_size: Optional[int] = 8192,\n",
    "        n_workers: Optional[int] = 8,\n",
    "    ) -> DiskDataset:\n",
    "        \"\"\"Create and return a `Dataset` object by featurizing provided files.\n",
    "\n",
    "        Reads in `inputs` and uses `self.featurizer` to featurize the\n",
    "        data in these input files.  For large files, automatically shards\n",
    "        into smaller chunks of `shard_size` datapoints for convenience.\n",
    "        Returns a `Dataset` object that contains the featurized dataset.\n",
    "\n",
    "        This implementation assumes that the helper methods `_get_shards`\n",
    "        and `_featurize_shard` are implemented and that each shard\n",
    "        returned by `_get_shards` is a pandas dataframe.  You may choose\n",
    "        to reuse or override this method in your subclass implementations.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        inputs : Sequence[Any]\n",
    "            List of inputs to process. Entries can be arbitrary objects so long as\n",
    "            they are understood by `self.featurizer`\n",
    "        data_dir : str, optional (default None)\n",
    "            Directory to store featurized dataset.\n",
    "        shard_size: int, optional (default 8192)\n",
    "            Number of examples stored in each shard.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        DiskDataset\n",
    "          A `DiskDataset` object containing a featurized representation of data\n",
    "          from `inputs`.\n",
    "        \"\"\"\n",
    "        logger.info(\"Loading raw samples now.\")\n",
    "        logger.info(\"shard_size: %s\" % str(shard_size))\n",
    "\n",
    "        if not isinstance(inputs, list):\n",
    "            try:\n",
    "                inputs = list(inputs)\n",
    "            except TypeError:\n",
    "                inputs = [inputs]\n",
    "\n",
    "        def _shard_generator():\n",
    "            global_index = 0  # noqa: F841\n",
    "            all_shard = [s for s in self._get_shards(inputs, shard_size)]\n",
    "            entry = 0\n",
    "            global_entry = [0]\n",
    "            for s in all_shard:\n",
    "                entry += len(s)\n",
    "                global_entry.append(entry)\n",
    "\n",
    "            with concurrent.futures.ProcessPoolExecutor(max_workers=n_workers) as executor:\n",
    "                time1 = time.time()\n",
    "                exe_results = executor.map(self._featurize_shard, all_shard, global_entry)\n",
    "                time2 = time.time()\n",
    "                logger.info(\"TIMING: featurizing shard took %0.3f s\" % (time2 - time1))\n",
    "            shard_results = [r for r in exe_results]\n",
    "\n",
    "            for sr in shard_results:\n",
    "                X, y, w, ids = sr[0], sr[1], sr[2], sr[3]\n",
    "                yield X, y, w, ids\n",
    "\n",
    "        return DiskDataset.create_dataset(_shard_generator(), data_dir, self.tasks)\n",
    "\n",
    "    def _get_shards(\n",
    "        self, inputs: List, shard_size: Optional[int]\n",
    "    ) -> Iterator[pd.DataFrame]:  # noqa: DAR301\n",
    "        \"\"\"Break up input into shards.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        inputs: List\n",
    "          Each entry in this list must be of the form `(featurization_input,\n",
    "          label, weight, id)` or `(featurization_input, label, weight)` or\n",
    "          `(featurization_input, label)` or `featurization_input` for one\n",
    "          datapoint, where `featurization_input` is any input that is recognized\n",
    "          by `self.featurizer`.\n",
    "        shard_size: int, optional\n",
    "          The size of shard to generate.\n",
    "\n",
    "        Yields\n",
    "        -------\n",
    "        Iterator[pd.DataFrame]\n",
    "          Iterator which iterates over shards of data.\n",
    "        \"\"\"\n",
    "        current_shard: List = []\n",
    "        for i, datapoint in enumerate(inputs):\n",
    "            if i != 0 and shard_size is not None and i % shard_size == 0:\n",
    "                shard_data = current_shard\n",
    "                current_shard = []\n",
    "                yield shard_data\n",
    "            current_shard.append(datapoint)\n",
    "        yield current_shard  # noqa: DAR301\n",
    "\n",
    "    # FIXME: Signature of \"_featurize_shard\" incompatible with supertype \"DataLoader\"\n",
    "    def _featurize_shard(  # type: ignore[override]\n",
    "        self, shard: List, global_index: List\n",
    "    ) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:  # noqa: DAR401\n",
    "        \"\"\"Featurizes a shard of an input data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        shard: List\n",
    "          List each entry of which must be of the form `(featurization_input,\n",
    "          label, weight, id)` or `(featurization_input, label, weight)` or\n",
    "          `(featurization_input, label)` or `featurization_input` for one\n",
    "          datapoint, where `featurization_input` is any input that is recognized\n",
    "          by `self.featurizer`.\n",
    "        global_index: int\n",
    "          The starting index for this shard in the full set of provided inputs\n",
    "\n",
    "        Returns\n",
    "        ------\n",
    "        Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]\n",
    "          The tuple is `(X, y, w, ids)`. All values are numpy arrays.\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        ValueError :\n",
    "          if entry has more than 4 elements.\n",
    "        \"\"\"\n",
    "        features = []\n",
    "        labels = []\n",
    "        weights = []\n",
    "        ids = []\n",
    "        n_tasks = len(self.tasks)\n",
    "        for _i, entry in enumerate(shard):\n",
    "            if not isinstance(entry, tuple):\n",
    "                entry = (entry,)\n",
    "            if len(entry) > 4:\n",
    "                raise ValueError(\n",
    "                    \"Entry is malformed and must be of length 1-4 containing featurization_input\"\n",
    "                    \"and optionally label, weight, and id.\"\n",
    "                )\n",
    "            if len(entry) == 4:\n",
    "                featurization_input, label, weight, entry_id = entry\n",
    "            elif len(entry) == 3:\n",
    "                featurization_input, label, weight = entry\n",
    "                entry_id = global_index\n",
    "            elif len(entry) == 2:\n",
    "                featurization_input, label = entry\n",
    "                weight = np.ones((n_tasks), np.float32)\n",
    "                entry_id = global_index\n",
    "            elif len(entry) == 1:\n",
    "                featurization_input = entry\n",
    "                label = np.zeros((n_tasks), np.float32)\n",
    "                weight = np.zeros((n_tasks), np.float32)\n",
    "                entry_id = global_index\n",
    "            feature = self.featurizer(featurization_input)\n",
    "            features.append(feature)\n",
    "            weights.append(weight)\n",
    "            labels.append(label)\n",
    "            ids.append(entry_id)\n",
    "        X = np.concatenate(features, axis=0)\n",
    "        return X, np.array(labels), np.array(weights), np.array(ids)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now implement a loader that will take the structures and labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StructureDataLoader(_MolnetLoader):\n",
    "    \"\"\"StructureDataLoader loader.\n",
    "\n",
    "    This data loader assumes that there is a folder with subfolder `cifs`.\n",
    "    The `cifs` subfolders contains all the cif files to be loaded.\n",
    "\n",
    "    Labels are loaded from a json-serialized file in the folder which\n",
    "    name can be specfied with `label_file_name`.\n",
    "\n",
    "    Note that there will be errors if the structures do not _exactly_ match\n",
    "    the entries in the json file.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    featurizer : Union[dc.feat.Featurizer, str]\n",
    "            the featurizer to use for processing the data.  Alternatively you can pass\n",
    "            one of the names from dc.molnet.featurizers as a shortcut.\n",
    "    loading_dir : str\n",
    "            path to the json where the dataset is stored. Should contain  a cif folders and a qmof.json file.\n",
    "    splitter : Union[dc.splits.Splitter, str], optional\n",
    "            the splitter to use for splitting the data into training, validation, and\n",
    "            test sets.  Alternatively you can pass one of the names from\n",
    "            dc.molnet.splitters as a shortcut.  If this is None, all the data\n",
    "            will be included in a single dataset.\n",
    "    transformer_generators : List[Union[TransformerGenerator, str]]\n",
    "            the Transformers to apply to the data.  Each one is specified by a\n",
    "            TransformerGenerator or, as a shortcut, one of the names from\n",
    "            dc.molnet.transformers.\n",
    "    tasks : List[str]\n",
    "            the names of the tasks in the dataset\n",
    "    data_dir : Optional[str]\n",
    "            a directory to save the raw data in\n",
    "    save_dir : Optional[str]\n",
    "            a directory to save the dataset in\n",
    "    label_file_name : Optional[str]\n",
    "            the name of the json file containing the labels. Defaults to `qmof.json`.\n",
    "    identifier_column : Optional[str]\n",
    "            the name of the column that contains the identifier of the structure.\n",
    "            Defaults to `qmof_id`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        structures: Iterable[Structure],\n",
    "        labels: Iterable[float],\n",
    "        idx: Iterable[int],\n",
    "        splitter: Union[str, dc.splits.Splitter] = \"random\",\n",
    "        transformer_generators: List[Union[str, TransformerGenerator]] = None,\n",
    "        tasks: List[str] = None,\n",
    "        data_dir: str = None,\n",
    "        save_dir: str = None,\n",
    "        number_files: int = np.infty,\n",
    "        shard_size: int = 8,\n",
    "        n_workers: int = 8,\n",
    "    ):\n",
    "        self.structures = structures\n",
    "        self.labels = labels\n",
    "        self.idx = idx\n",
    "\n",
    "        self.number_files = number_files\n",
    "\n",
    "        # keep it here such that we can reuse it later\n",
    "        self.dataset_file = join(self.loading_dir, \"cifs\")\n",
    "\n",
    "        self.shard_size = shard_size\n",
    "        self.n_workers = n_workers\n",
    "\n",
    "        super().__init__(\n",
    "            featurizer=CGCNNFeaturizer(),\n",
    "            splitter=splitter,\n",
    "            transformer_generators=transformer_generators,\n",
    "            tasks=tasks,\n",
    "            data_dir=data_dir,\n",
    "            save_dir=save_dir,\n",
    "        )\n",
    "\n",
    "    def create_dataset(self) -> Dataset:\n",
    "        \"\"\"Utilitary function to create the dataset.\"\"\"\n",
    "        loader = InMemoryLoader(\n",
    "            tasks=self.tasks,\n",
    "            featurizer=self.featurizer,\n",
    "        )\n",
    "\n",
    "        weights = [np.ones((len(loader.tasks)), np.float32)] * len(self.structures)\n",
    "\n",
    "        return loader.create_dataset(\n",
    "            list(zip(self.structure, self.labels, weights, self.idx)),\n",
    "            shard_size=self.shard_size,\n",
    "            n_workers=self.n_workers,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check our plumbing works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('mofdscribe')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3ffc06f754d7c80b59e39914e7792f1f92938dc6ca13a8ff96847f8f4d27fee3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
