{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to submit to the leaderboard\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we will build a graph-convolutional neural network for the PBE bandgap task.\n",
    "We will use the [Crystal Graph Convolutional Neural Network](https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.120.145301) implemented in the [Deepchem library](https://deepchem.io/).\n",
    "\n",
    "We choose this one, as it will need some more involved training loop and customization that you might also need in a real-world scenario.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To install Deepchem, follow the installation instructions on [the Deepchem landing page](https://deepchem.io/). You'll need to run something along the following lines (and also install PyTorch and dgl):\n",
    "\n",
    "> conda install -c conda-forge rdkit deepchem==2.6.1\n",
    ">\n",
    "> pip install tensorflow-gpu~=2.4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepchem.data import Dataset, DiskDataset\n",
    "from deepchem.data.data_loader import InMemoryLoader, DataLoader\n",
    "from deepchem.feat import MaterialStructureFeaturizer\n",
    "from deepchem.feat.graph_data import GraphData\n",
    "from deepchem.models.torch_models.cgcnn import CGCNNModel\n",
    "from deepchem.molnet.load_function.molnet_loader import TransformerGenerator, _MolnetLoader\n",
    "from deepchem.utils.data_utils import download_url, get_data_dir\n",
    "from deepchem.utils.typing import PymatgenStructure\n",
    "from loguru import logger\n",
    "from mofdscribe.bench import PBEBandGapBench\n",
    "from pymatgen.analysis.local_env import CrystalNN, CutOffDictNN, JmolNN\n",
    "from pymatgen.core import Structure\n",
    "from pymatgen.core.structure import Structure\n",
    "from typing import List, Tuple, Union, Iterable, Sequence, Optional, Any, Iterator\n",
    "from typing import Tuple\n",
    "\n",
    "import deepchem as dc\n",
    "import json\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import concurrent.futures\n",
    "import time\n",
    "import os\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "ATOM_INIT_JSON_URL = \"https://deepchemdata.s3-us-west-1.amazonaws.com/datasets/atom_init.json\"\n",
    "VESTA_NN = CutOffDictNN.from_preset(\"vesta_2019\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plumbing to make Deepchem work with custom datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CGCNNModel(mode=\"regression\", in_edge_dim=16)  # we use default settings for demonstration purposes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we have a custom dataloader, which also supports multiprocessing (we will use this one in the Python script, in which we run it for a larger dataset). In this notebook, we will use the default `InMemoryLoader`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class InMemoryLoader(DataLoader):\n",
    "#     \"\"\"Facilitate Featurization of In-memory objects.\n",
    "\n",
    "#     When featurizing a dataset, it's often the case that the initial set of\n",
    "#     data (pre-featurization) fits handily within memory. (For example, perhaps\n",
    "#     it fits within a column of a pandas DataFrame.) In this case, it would be\n",
    "#     convenient to directly be able to featurize this column of data. However,\n",
    "#     the process of featurization often generates large arrays which quickly eat\n",
    "#     up available memory. This class provides convenient capabilities to process\n",
    "#     such in-memory data by checkpointing generated features periodically to\n",
    "#     disk.\n",
    "\n",
    "#     Example\n",
    "#     -------\n",
    "#     Here's an example with only datapoints and no labels or weights.\n",
    "\n",
    "#     >>> import deepchem as dc\n",
    "#     >>> smiles = [\"C\", \"CC\", \"CCC\", \"CCCC\"]\n",
    "#     >>> featurizer = dc.feat.CircularFingerprint()\n",
    "#     >>> loader = dc.data.InMemoryLoader(tasks=[\"task1\"], featurizer=featurizer)\n",
    "#     >>> dataset = loader.create_dataset(smiles, shard_size=2)\n",
    "#     >>> len(dataset)\n",
    "#     4\n",
    "\n",
    "#     Here's an example with both datapoints and labels\n",
    "\n",
    "#     >>> import deepchem as dc\n",
    "#     >>> smiles = [\"C\", \"CC\", \"CCC\", \"CCCC\"]\n",
    "#     >>> labels = [1, 0, 1, 0]\n",
    "#     >>> featurizer = dc.feat.CircularFingerprint()\n",
    "#     >>> loader = dc.data.InMemoryLoader(tasks=[\"task1\"], featurizer=featurizer)\n",
    "#     >>> dataset = loader.create_dataset(zip(smiles, labels), shard_size=2)\n",
    "#     >>> len(dataset)\n",
    "#     4\n",
    "\n",
    "#     \"\"\"\n",
    "\n",
    "#     def create_dataset(\n",
    "#         self,\n",
    "#         inputs: Sequence[Any],\n",
    "#         data_dir: Optional[str] = None,\n",
    "#         shard_size: Optional[int] = 8192,\n",
    "#         n_workers: Optional[int] = 8,\n",
    "#     ) -> DiskDataset:\n",
    "#         \"\"\"Create and return a `Dataset` object by featurizing provided files.\n",
    "\n",
    "#         Reads in `inputs` and uses `self.featurizer` to featurize the\n",
    "#         data in these input files.  For large files, automatically shards\n",
    "#         into smaller chunks of `shard_size` datapoints for convenience.\n",
    "#         Returns a `Dataset` object that contains the featurized dataset.\n",
    "\n",
    "#         This implementation assumes that the helper methods `_get_shards`\n",
    "#         and `_featurize_shard` are implemented and that each shard\n",
    "#         returned by `_get_shards` is a pandas dataframe.  You may choose\n",
    "#         to reuse or override this method in your subclass implementations.\n",
    "\n",
    "#         Parameters\n",
    "#         ----------\n",
    "#         inputs : Sequence[Any]\n",
    "#             List of inputs to process. Entries can be arbitrary objects so long as\n",
    "#             they are understood by `self.featurizer`\n",
    "#         data_dir : str, optional (default None)\n",
    "#             Directory to store featurized dataset.\n",
    "#         shard_size: int, optional (default 8192)\n",
    "#             Number of examples stored in each shard.\n",
    "\n",
    "#         Returns\n",
    "#         -------\n",
    "#         DiskDataset\n",
    "#           A `DiskDataset` object containing a featurized representation of data\n",
    "#           from `inputs`.\n",
    "#         \"\"\"\n",
    "#         logger.info(\"Loading raw samples now.\")\n",
    "#         logger.info(\"shard_size: %s\" % str(shard_size))\n",
    "\n",
    "#         if not isinstance(inputs, list):\n",
    "#             try:\n",
    "#                 inputs = list(inputs)\n",
    "#             except TypeError:\n",
    "#                 inputs = [inputs]\n",
    "\n",
    "#         def _shard_generator():\n",
    "#             global_index = 0  # noqa: F841\n",
    "#             all_shard = [s for s in self._get_shards(inputs, shard_size)]\n",
    "#             entry = 0\n",
    "#             global_entry = [0]\n",
    "#             for s in all_shard:\n",
    "#                 entry += len(s)\n",
    "#                 global_entry.append(entry)\n",
    "\n",
    "#             with concurrent.futures.ProcessPoolExecutor(max_workers=n_workers) as executor:\n",
    "#                 time1 = time.time()\n",
    "#                 exe_results = executor.map(self._featurize_shard, all_shard, global_entry)\n",
    "#                 time2 = time.time()\n",
    "#                 logger.info(\"TIMING: featurizing shard took %0.3f s\" % (time2 - time1))\n",
    "#             shard_results = [r for r in exe_results]\n",
    "\n",
    "#             for sr in shard_results:\n",
    "#                 X, y, w, ids = sr[0], sr[1], sr[2], sr[3]\n",
    "#                 yield X, y, w, ids\n",
    "\n",
    "#         return DiskDataset.create_dataset(_shard_generator(), data_dir, self.tasks)\n",
    "\n",
    "#     def _get_shards(\n",
    "#         self, inputs: List, shard_size: Optional[int]\n",
    "#     ) -> Iterator[pd.DataFrame]:  # noqa: DAR301\n",
    "#         \"\"\"Break up input into shards.\n",
    "\n",
    "#         Parameters\n",
    "#         ----------\n",
    "#         inputs: List\n",
    "#           Each entry in this list must be of the form `(featurization_input,\n",
    "#           label, weight, id)` or `(featurization_input, label, weight)` or\n",
    "#           `(featurization_input, label)` or `featurization_input` for one\n",
    "#           datapoint, where `featurization_input` is any input that is recognized\n",
    "#           by `self.featurizer`.\n",
    "#         shard_size: int, optional\n",
    "#           The size of shard to generate.\n",
    "\n",
    "#         Yields\n",
    "#         -------\n",
    "#         Iterator[pd.DataFrame]\n",
    "#           Iterator which iterates over shards of data.\n",
    "#         \"\"\"\n",
    "#         current_shard: List = []\n",
    "#         for i, datapoint in enumerate(inputs):\n",
    "#             if i != 0 and shard_size is not None and i % shard_size == 0:\n",
    "#                 shard_data = current_shard\n",
    "#                 current_shard = []\n",
    "#                 yield shard_data\n",
    "#             current_shard.append(datapoint)\n",
    "#         yield current_shard  # noqa: DAR301\n",
    "\n",
    "#     # FIXME: Signature of \"_featurize_shard\" incompatible with supertype \"DataLoader\"\n",
    "#     def _featurize_shard(  # type: ignore[override]\n",
    "#         self, shard: List, global_index: List\n",
    "#     ) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:  # noqa: DAR401\n",
    "#         \"\"\"Featurizes a shard of an input data.\n",
    "\n",
    "#         Parameters\n",
    "#         ----------\n",
    "#         shard: List\n",
    "#           List each entry of which must be of the form `(featurization_input,\n",
    "#           label, weight, id)` or `(featurization_input, label, weight)` or\n",
    "#           `(featurization_input, label)` or `featurization_input` for one\n",
    "#           datapoint, where `featurization_input` is any input that is recognized\n",
    "#           by `self.featurizer`.\n",
    "#         global_index: int\n",
    "#           The starting index for this shard in the full set of provided inputs\n",
    "\n",
    "#         Returns\n",
    "#         ------\n",
    "#         Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]\n",
    "#           The tuple is `(X, y, w, ids)`. All values are numpy arrays.\n",
    "\n",
    "#         Raises\n",
    "#         ------\n",
    "#         ValueError :\n",
    "#           if entry has more than 4 elements.\n",
    "#         \"\"\"\n",
    "#         features = []\n",
    "#         labels = []\n",
    "#         weights = []\n",
    "#         ids = []\n",
    "#         n_tasks = len(self.tasks)\n",
    "#         for _i, entry in enumerate(shard):\n",
    "#             if not isinstance(entry, tuple):\n",
    "#                 entry = (entry,)\n",
    "#             if len(entry) > 4:\n",
    "#                 raise ValueError(\n",
    "#                     \"Entry is malformed and must be of length 1-4 containing featurization_input\"\n",
    "#                     \"and optionally label, weight, and id.\"\n",
    "#                 )\n",
    "#             if len(entry) == 4:\n",
    "#                 featurization_input, label, weight, entry_id = entry\n",
    "#             elif len(entry) == 3:\n",
    "#                 featurization_input, label, weight = entry\n",
    "#                 entry_id = global_index\n",
    "#             elif len(entry) == 2:\n",
    "#                 featurization_input, label = entry\n",
    "#                 weight = np.ones((n_tasks), np.float32)\n",
    "#                 entry_id = global_index\n",
    "#             elif len(entry) == 1:\n",
    "#                 featurization_input = entry\n",
    "#                 label = np.zeros((n_tasks), np.float32)\n",
    "#                 weight = np.zeros((n_tasks), np.float32)\n",
    "#                 entry_id = global_index\n",
    "#             feature = self.featurizer(featurization_input)\n",
    "#             features.append(feature)\n",
    "#             weights.append(weight)\n",
    "#             labels.append(label)\n",
    "#             ids.append(entry_id)\n",
    "#         X = np.concatenate(features, axis=0)\n",
    "#         return X, np.array(labels), np.array(weights), np.array(ids)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a simpler featurizer, however, you can also use the default `CGCNNFeaturizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrystalBondFeaturizer(MaterialStructureFeaturizer):\n",
    "    \"\"\"\n",
    "    Calculate structure graph features for crystals.\n",
    "\n",
    "    Based on the implementation in Crystal Graph Convolutional\n",
    "    Neural Networks (CGCNN). The method constructs a crystal graph\n",
    "    representation including atom features and bond features (neighbor\n",
    "    distances). Neighbors are determined using bond heuristics.\n",
    "    Optionally, a Gaussian filter is applied to neighbor distances.\n",
    "    All units are in Angstrom.\n",
    "    This featurizer requires the optional dependency pymatgen. It may\n",
    "    be useful when 3D coordinates are available and when using graph\n",
    "    network models and crystal graph convolutional networks.\n",
    "    See [1]_ for more details.\n",
    "    References\n",
    "    ----------\n",
    "    .. [1] T. Xie and J. C. Grossman, \"Crystal graph convolutional\n",
    "       neural networks for an accurate and interpretable prediction\n",
    "       of material properties\", Phys. Rev. Lett. 120, 2018,\n",
    "       https://arxiv.org/abs/1710.10324\n",
    "    Examples\n",
    "    --------\n",
    "    >>> import pymatgen as mg\n",
    "    >>> featurizer = CrystalBondFeaturizer()\n",
    "    >>> lattice = mg.core.Lattice.cubic(4.2)\n",
    "    >>> structure = mg.core.Structure(lattice, [\"Cs\", \"Cl\"], [[0, 0, 0], [0.5, 0.5, 0.5]])\n",
    "    >>> features = featurizer.featurize([structure])\n",
    "    >>> feature = features[0]\n",
    "    >>> print(type(feature))\n",
    "    <class 'deepchem.feat.graph_data.GraphData'>\n",
    "    Note\n",
    "    ----\n",
    "    This class requires Pymatgen to be installed.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, heuristic: str = \"vesta\", step: float = 0.2, radius: float = 3.0):\n",
    "        \"\"\"Initialize CrystalBondFeaturizer.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        heuristic : str\n",
    "            The heuristic to use for determining neighbors.\n",
    "        radius : float\n",
    "            Radius of sphere for finding neighbors of atoms in unit cell. This is the radius\n",
    "            of the Gaussian filter. Default is 3.0.\n",
    "        step : float\n",
    "            Step size for Gaussian filter. This value is used when building edge features.\n",
    "            If None, use only the bond length. Default is 0.2.\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        ValueError\n",
    "            If `heuristic` is not one of the following: [\"vesta\", \"jmol\", \"crystal\"].\n",
    "        \"\"\"\n",
    "        heuristic = heuristic.lower()\n",
    "        if heuristic == \"vesta\":\n",
    "            self.nn = VESTA_NN\n",
    "        elif heuristic == \"jmol\":\n",
    "            self.nn = JmolNN()\n",
    "        elif heuristic == \"crystal\":\n",
    "            self.nn = CrystalNN()\n",
    "        else:\n",
    "            raise ValueError(\"Unknown heuristic: {}\".format(heuristic))\n",
    "        self.step = step\n",
    "        self.radius = radius\n",
    "        # load atom_init.json\n",
    "        data_dir = get_data_dir()\n",
    "        download_url(ATOM_INIT_JSON_URL, data_dir)\n",
    "        atom_init_json_path = os.path.join(data_dir, \"atom_init.json\")\n",
    "        with open(atom_init_json_path, \"r\") as f:\n",
    "            atom_init_json = json.load(f)\n",
    "\n",
    "        self.atom_features = {\n",
    "            int(key): np.array(value, dtype=np.float32) for key, value in atom_init_json.items()\n",
    "        }\n",
    "        self.valid_atom_number = set(self.atom_features.keys())\n",
    "\n",
    "    def _featurize(self, datapoint: PymatgenStructure, **kwargs) -> GraphData:\n",
    "        \"\"\"Calculate crystal graph features from pymatgen structure.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        datapoint: pymatgen.core.Structure\n",
    "                A periodic crystal composed of a lattice and a sequence of atomic\n",
    "                sites with 3D coordinates and elements.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        graph: GraphData\n",
    "                A crystal graph with CGCNN style features.\n",
    "        \"\"\"\n",
    "        if type(datapoint) is not Structure:\n",
    "            logger.warning(\n",
    "                f\"CrystalBondFeaturizer requires pymatgen.core.Structure, got {type(datapoint)}\"\n",
    "            )\n",
    "            raise ValueError(\n",
    "                f\"CrystalBondFeaturizer requires pymatgen.core.Structure, got {type(datapoint)}\"\n",
    "            )\n",
    "        node_features = self._get_node_features(datapoint)\n",
    "        edge_index, edge_features = self._get_edge_features_and_index(datapoint)\n",
    "        graph = GraphData(node_features, edge_index, edge_features)\n",
    "        return graph\n",
    "\n",
    "    def _get_node_features(self, struct: PymatgenStructure) -> np.ndarray:\n",
    "        \"\"\"Get the node feature from `atom_init.json`.\n",
    "\n",
    "        The `atom_init.json` was collected\n",
    "        from `data/sample-regression/atom_init.json` in the CGCNN repository.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        struct: pymatgen.core.Structure\n",
    "            A periodic crystal composed of a lattice and a sequence of atomic\n",
    "            sites with 3D coordinates and elements.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        node_features: np.ndarray\n",
    "            A numpy array of shape `(num_nodes, 92)`.\n",
    "        \"\"\"\n",
    "        node_features = []\n",
    "        for site in struct:\n",
    "            # check whether the atom feature exists or not\n",
    "            if site.specie.number not in self.valid_atom_number:\n",
    "                raise RuntimeError(\"site.specie.number not in self.valid_atom_number\")\n",
    "            node_features.append(self.atom_features[site.specie.number])\n",
    "        return np.vstack(node_features).astype(float)\n",
    "\n",
    "    def _get_edge_features_and_index(\n",
    "        self, struct: PymatgenStructure\n",
    "    ) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"Calculate the edge feature and edge index from pymatgen structure.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        struct: pymatgen.core.Structure\n",
    "            A periodic crystal composed of a lattice and a sequence of atomic\n",
    "            sites with 3D coordinates and elements.\n",
    "        Returns\n",
    "        -------\n",
    "        edge_idx: np.ndarray, dtype int\n",
    "            A numpy array of shape with `(2, num_edges)`.\n",
    "        edge_features: np.ndarray\n",
    "            A numpy array of shape with `(num_edges, filter_length)`. The `filter_length` is\n",
    "            (self.radius / self.step) + 1. The edge features were built by applying gaussian\n",
    "            filter to the distance between nodes.\n",
    "        \"\"\"\n",
    "        neighbors_ = self.nn.get_all_nn_info(struct)\n",
    "\n",
    "        neighbors = []\n",
    "        for n in neighbors_:\n",
    "            sites = [s[\"site\"] for s in n]\n",
    "            n = sorted(sites, key=lambda x: x[1])\n",
    "            neighbors.append(n)\n",
    "\n",
    "        # construct bi-directed graph\n",
    "        src_idx, dest_idx = [], []\n",
    "        edge_distances = []\n",
    "        for node_idx, neighbor in enumerate(neighbors):\n",
    "            src_idx.extend([node_idx] * len(neighbor))\n",
    "            dest_idx.extend([site[2] for site in neighbor])\n",
    "            edge_distances.extend([site[1] for site in neighbor])\n",
    "\n",
    "        edge_idx = np.array([src_idx, dest_idx], dtype=int)\n",
    "\n",
    "        if self.step is None:\n",
    "            edge_features = np.array(edge_distances)\n",
    "        else:\n",
    "            edge_features = self._gaussian_filter(np.array(edge_distances, dtype=float))\n",
    "        return edge_idx, edge_features\n",
    "\n",
    "    def _gaussian_filter(self, distances: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Apply Gaussian filter to an array of interatomic distances.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        distances : np.ndarray\n",
    "                A numpy array of the shape `(num_edges, )`.\n",
    "        Returns\n",
    "        -------\n",
    "        expanded_distances: np.ndarray\n",
    "                Expanded distance tensor after Gaussian filtering.\n",
    "                The shape is `(num_edges, filter_length)`. The `filter_length` is\n",
    "                (self.radius / self.step) + 1.\n",
    "        \"\"\"\n",
    "        filt = np.arange(0, self.radius + self.step, self.step)\n",
    "\n",
    "        # Increase dimension of distance tensor and apply filter\n",
    "        expanded_distances = np.exp(-((distances[..., np.newaxis] - filt) ** 2) / self.step ** 2)\n",
    "\n",
    "        return expanded_distances\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now implement a loader that will take the structures and labels. Some notes:\n",
    "\n",
    "- we convert to lists as the splitters by default return generators.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StructureDataLoader(_MolnetLoader):\n",
    "    \"\"\"StructureDataLoader loader.\n",
    "\n",
    "    This data loader assumes that there is a folder with subfolder `cifs`.\n",
    "    The `cifs` subfolders contains all the cif files to be loaded.\n",
    "\n",
    "    Labels are loaded from a json-serialized file in the folder which\n",
    "    name can be specfied with `label_file_name`.\n",
    "\n",
    "    Note that there will be errors if the structures do not _exactly_ match\n",
    "    the entries in the json file.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    featurizer : Union[dc.feat.Featurizer, str]\n",
    "            the featurizer to use for processing the data.  Alternatively you can pass\n",
    "            one of the names from dc.molnet.featurizers as a shortcut.\n",
    "    splitter : Union[dc.splits.Splitter, str], optional\n",
    "            the splitter to use for splitting the data into training, validation, and\n",
    "            test sets.  Alternatively you can pass one of the names from\n",
    "            dc.molnet.splitters as a shortcut.  If this is None, all the data\n",
    "            will be included in a single dataset.\n",
    "    transformer_generators : List[Union[TransformerGenerator, str]]\n",
    "            the Transformers to apply to the data.  Each one is specified by a\n",
    "            TransformerGenerator or, as a shortcut, one of the names from\n",
    "            dc.molnet.transformers.\n",
    "    tasks : List[str]\n",
    "            the names of the tasks in the dataset\n",
    "    data_dir : Optional[str]\n",
    "            a directory to save the raw data in\n",
    "    save_dir : Optional[str]\n",
    "            a directory to save the dataset in\n",
    "    label_file_name : Optional[str]\n",
    "            the name of the json file containing the labels. Defaults to `qmof.json`.\n",
    "    identifier_column : Optional[str]\n",
    "            the name of the column that contains the identifier of the structure.\n",
    "            Defaults to `qmof_id`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        structures: Iterable[Structure],\n",
    "        labels: Iterable[float],\n",
    "        idx: Iterable[int],\n",
    "        splitter: Union[str, dc.splits.Splitter] = None,#\"random\",\n",
    "        transformer_generators: List[Union[str, TransformerGenerator]] = [\"normalization\"],\n",
    "        tasks: List[str] = ['qmof'],\n",
    "        data_dir: str = None,\n",
    "        save_dir: str = None,\n",
    "        number_files: int = np.infty,\n",
    "        shard_size: int = 8,\n",
    "        n_workers: int = 1,\n",
    "    ):\n",
    "        # we return IStructures, however, the featurizer wants structures.\n",
    "        self.structures = [[Structure.from_sites(s.sites)] for s in structures]\n",
    "        self.labels = np.array(list(labels)).reshape(-1,1)\n",
    "        self.idx = np.array(list(idx))\n",
    "\n",
    "        self.number_files = number_files\n",
    "\n",
    "        self.shard_size = shard_size\n",
    "        self.n_workers = n_workers\n",
    "\n",
    "        super().__init__(\n",
    "            featurizer=CrystalBondFeaturizer(),\n",
    "            splitter=splitter,\n",
    "            transformer_generators=transformer_generators,\n",
    "            tasks=tasks,\n",
    "            data_dir=data_dir,\n",
    "            save_dir=save_dir,\n",
    "        )\n",
    "\n",
    "    def create_dataset(self) -> Dataset:\n",
    "        \"\"\"Utilitary function to create the dataset.\"\"\"\n",
    "        loader = InMemoryLoader(\n",
    "            tasks=self.tasks,\n",
    "            featurizer=self.featurizer,\n",
    "        )\n",
    "        \n",
    "        return loader.create_dataset(\n",
    "            list(zip(self.structures, self.labels)),\n",
    "            shard_size=self.shard_size,\n",
    "            #     n_workers=self.n_workers,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check our plumbing works.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initialize a bench class to access the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-02 11:53:28.297 | DEBUG    | mofdscribe.datasets.qmof_dataset:__init__:120 - Dropped 0 duplicate basenames. New length 15844\n",
      "2022-08-02 11:53:28.313 | DEBUG    | mofdscribe.datasets.qmof_dataset:__init__:126 - Dropped 153 duplicate graphs. New length 15691\n",
      "2022-08-02 11:53:28.899 | DEBUG    | mofdscribe.datasets.qmof_dataset:__init__:120 - Dropped 0 duplicate basenames. New length 15844\n",
      "2022-08-02 11:53:28.915 | DEBUG    | mofdscribe.datasets.qmof_dataset:__init__:126 - Dropped 153 duplicate graphs. New length 15691\n",
      "2022-08-02 11:53:28.965 | DEBUG    | mofdscribe.splitters.splitters:__init__:106 - Splitter settings | shuffle True, random state None, sample frac 0.01, q (0, 0.25, 0.5, 0.75, 1)\n"
     ]
    }
   ],
   "source": [
    "bench = PBEBandGapBench(None, \"test\", debug=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get some random indices\n",
    "indices = np.random.choice(np.arange(len(bench._ds)), 10)\n",
    "\n",
    "structures = bench._ds.get_structures(indices)\n",
    "labels = bench._ds._df[bench._targets].iloc[indices].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = StructureDataLoader(structures, labels, indices, data_dir=\"test-data\", save_dir='test')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = loader.load_dataset(\"test\", True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['qmof'],\n",
       " (<DiskDataset X.shape: (10,), y.shape: (10, 1), w.shape: (10, 1), ids: [0 0 0 0 0 0 0 0 8 8], task_names: ['qmof']>,),\n",
       " [<deepchem.trans.transformers.NormalizationTransformer at 0x2b2878eb0>])"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevinmaikjablonka/miniconda3/envs/mofdscribe/lib/python3.8/site-packages/torch/autocast_mode.py:162: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.050350230932235715"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(ds[1][0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing a model class for `Bench`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, that we now that the plumbing works, we only have to wrap it into a class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CGCNNModel:\n",
    "    def __init__(self, model): \n",
    "        self.model = model\n",
    "        self.scaler = None \n",
    "        self._fitted = False \n",
    "\n",
    "    def _create_ds(): \n",
    "        ...\n",
    "\n",
    "    def fit(self, idx, structures, y):\n",
    "        ...\n",
    "    \n",
    "    def predict(self, idx, structures):\n",
    "        if not self._fitted: \n",
    "            raise Exception(\"Model not fitted\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('mofdscribe')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3ffc06f754d7c80b59e39914e7792f1f92938dc6ca13a8ff96847f8f4d27fee3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
